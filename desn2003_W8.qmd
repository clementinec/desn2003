---
title: "Research Methodologies in Design, HCI, and Generative AI"
subtitle: "DESN2003: Research for Innovation, Week Eight (Part Two)"
author: Hongshan Guo
format: 
  revealjs:
    slide-level: 2
    center: true
    slide-number: true
    theme: simple
    width: 1600
    height: 1200
    preview-links: auto
    embed-resources: true
    incremental: true
---
## 

**Exploring Paradigms & Methods** – An academically grounded lecture on how to research for design innovation and emerging tech.

::: {.notes}
**Welcome & Context:** This lecture covers research methodologies from a design and HCI perspective, with a focus on emerging technologies like Generative AI. We’ll bridge theory (paradigms, epistemology) with practical methods (qualitative, quantitative, design-based) and real-world examples. The goal is to equip you – future product managers and data scientists – with a solid foundation in how to plan and conduct research for user-centric design and innovation.
**Housekeeping:** This is a 2-hour session with interactive components. We’ll have two activities (~30 min total) to apply concepts. Feel free to ask questions as we go!
:::

## Why Methodology Matters in Design & Tech

- **Building the Right Thing:** Research helps uncover user needs, context, and whether our design/AI solution actually solves the real problem.
- **Building the Thing Right:** Rigorous methods validate design decisions – ensuring usability, usefulness, and a good user experience with evidence.
- **Navigating Uncertainty:** Emerging tech (like GenAI) is novel – systematic inquiry guides us when intuition isn’t enough.
- **Interdisciplinary Collaboration:** Knowing research fundamentals lets designers, product managers, and data scientists speak a common language when evaluating ideas.

::: {.notes}
**Talking Points:** In design and tech projects, especially with cutting-edge tech, it’s easy to make wrong assumptions. Research methodology is our toolkit for de-risking innovation:
- *Building the right thing:* We use research to ensure we're solving the right user problem. For example, before coding a generative AI feature, we might interview users about their needs.
- *Building the thing right:* Once we have an idea, research (like usability testing or experiments) verifies that our implementation actually works for users.
- *Uncertainty:* New tech (say a new AI tool) has no blueprint. Research provides a systematic way to explore unknowns (what will users do with it? What issues arise?).
- *Collaboration:* In product teams, designers, PMs, engineers, data scientists all need to make evidence-based decisions. Understanding research methods helps everyone align on facts rather than opinions.
This sets the stage for why we need a strong grounding in research methods.
:::

## Lecture Roadmap

1. **Research Paradigms & Epistemology:** Worldviews guiding research (positivist vs. constructivist, etc.) and why they matter.
2. **Qualitative & Quantitative Methods:** Key approaches, when to use each, and how they differ and complement each other.
3. **Design-Based Research:** Iterative design-as-research methods and how they apply in HCI and product development.
4. **Case Studies & Examples:** Real-world scenarios (including GenAI) illustrating methodologies in action.
5. **Activities:** Interactive exercises to apply paradigms and plan research for a GenAI design scenario.
6. **Takeaways:** Summarize best practices for rigorous yet practical research in design and emerging tech.

::: {.notes}
**Guide through outline:** 
1. *Paradigms:* We’ll start with the “big picture” – philosophical stances like positivism and constructivism that influence how we do research.
2. *Qual vs Quant:* Then we dive into methods – qualitative versus quantitative research, their tools and roles.
3. *Design-Based Research:* We’ll discuss a hybrid approach where designing and researching happen together (important in fields like education and HCI).
4. *Examples:* Throughout, I’ll bring in examples (both industry cases and maybe research studies) to ground theory in practice. We have a special eye on generative AI as an example of emerging tech.
5. *Activities:* Two planned activities – one smaller one soon to identify paradigms in scenarios, and a longer one later where you’ll sketch a research plan (with possibly a quick Figma prototype) for an AI product.
6. *Conclusion:* Finally, key takeaways to remember.
Feel free to note questions for discussion at end or during activities.
:::

## Research Paradigms: Theoretical Lenses

- **What’s a Paradigm?** A fundamental worldview or belief system about knowledge and reality that guides research approach ([Positivist and Constructivist Paradigms | What do you think you're doing?](https://applingtesol.wordpress.com/2019/10/30/positivist-and-constructivist-paradigms/#:~:text=,methodology)) ([Positivist and Constructivist Paradigms | What do you think you're doing?](https://applingtesol.wordpress.com/2019/10/30/positivist-and-constructivist-paradigms/#:~:text=Positivism%20is%20a%20research%20paradigm,to%20test%20theories%20or%20hypotheses)). It includes:
  
  - **Ontology:** What is reality? (e.g., is there one truth or many perspectives?)
  - **Epistemology:** How do we know things? (e.g., through objective measurement or subjective understanding?)
  - **Methodology:** How do we systematically investigate?
- **Why Paradigms Matter:** They shape the questions we ask, the methods we choose, and how we interpret findings. Two key paradigms in our context:
  - *Positivist* (traditional scientific) vs. *Constructivist* (interpretive) – a classic contrast.
  - (Plus other paradigms like *Critical* or *Pragmatic*, which we’ll touch on briefly.)

::: {.notes}
**Define Paradigm:** A paradigm is like a research worldview or mindset. It’s composed of:
- Ontology: beliefs about reality. For example, a positivist assumes an objective reality exists independently, while a constructivist believes reality is constructed in people’s minds (multiple realities).
- Epistemology: beliefs about knowledge. Positivist thinks we gain knowledge by objective measurement and testing hypotheses; constructivist thinks we gain knowledge by understanding subjective experiences and context.
- Methodology: given those beliefs, the approaches we use (e.g., experiments vs. interviews).

**Importance:** If you approach a design problem as a positivist, you might run controlled experiments and look for measurable improvements. As a constructivist, you might do field studies, trying to understand user narratives. Paradigms influence everything from research questions to data analysis. We highlight positivism and constructivism as they’re foundational extremes – many projects blend elements (pragmatism is common in industry, meaning use what works). We’ll explore these in the next slides.
:::

## Positivist Paradigm (Objectivist)

- **Key Idea:** Reality exists objectively “out there” – by studying it systematically, we can discover universal truths or laws.
- **Research Aim:** Investigate, confirm, and predict law-like patterns of behavior ([Positivist and Constructivist Paradigms | What do you think you're doing?](https://applingtesol.wordpress.com/2019/10/30/positivist-and-constructivist-paradigms/#:~:text=Positivism%20is%20a%20research%20paradigm,to%20test%20theories%20or%20hypotheses)). Often involves testing hypotheses and theories in controlled ways.
- **Characteristics:** Stresses objectivity and researcher detachment; relies on quantifiable observations and formal experiments ([Positivist and Constructivist Paradigms | What do you think you're doing?](https://applingtesol.wordpress.com/2019/10/30/positivist-and-constructivist-paradigms/#:~:text=attribute%20to%20the%20views%20of,quantitative%20methodology%2C%20utilizing%20experimental%20methods%E2%80%9D)).
- **Common Methods:** Quantitative methods dominate – e.g., lab experiments, surveys with statistical analysis, A/B tests with large samples.
- **Example in Design:** Running a controlled A/B test for two UI designs with thousands of users to objectively determine which design yields higher conversion (seeking a generalizable “best” design).

::: {.notes}
**Explain Positivism in plain terms:** This is the classic scientific approach. Think physics or classical usability studies:
- There’s a belief that there is one reality/truth (e.g., a *best* way to design an interface that maximizes efficiency).
- The researcher tries to be objective, like a neutral observer. We often separate ourselves from participants (one-way mirrors in usability labs, etc).
- We form a hypothesis (“Design A will be faster than Design B”), then gather data (like task completion times) to confirm or refute it.
- Because of the focus on objectivity, methods are typically quantitative: metrics, statistical tests, large sample sizes to ensure the findings aren’t just due to chance.
- **Example expansion:** Suppose a product manager is very positivist in mindset. They might say “Let’s not rely on anecdotes, let’s run an experiment. 10,000 users get the old feature, 10,000 get the new AI feature, and we measure engagement. Whichever has higher engagement wins.” That’s a very objectivist, data-driven approach aligned with positivism.
:::

## Constructivist Paradigm (Interpretivist)

- **Key Idea:** Reality is subjective and socially constructed – different people may have different valid interpretations of the world ([Positivist and Constructivist Paradigms | What do you think you're doing?](https://applingtesol.wordpress.com/2019/10/30/positivist-and-constructivist-paradigms/#:~:text=Lincoln%20and%20Guba%E2%80%99s%20,pluralist%20and%20relativist)).
- **Research Aim:** Understand the meanings and experiences of participants in context, rather than finding one “universal law.” Truth is relative to perspective and context.
- **Characteristics:** Researcher is an integral part of the process (cannot fully detach observer from observed) ([Positivist and Constructivist Paradigms | What do you think you're doing?](https://applingtesol.wordpress.com/2019/10/30/positivist-and-constructivist-paradigms/#:~:text=The%20observer%20cannot%20be%20neatly,in%20the%20minds%20of%20individuals)); knowledge is co-created through interaction.
- **Common Methods:** Qualitative methods prevail – e.g., ethnography, interviews, case studies, participant observation – yielding rich descriptions.
- **Example in Design:** Conducting in-depth field studies of how various users adapt a new generative AI tool in their daily workflow, to grasp diverse experiences and contexts (no single metric, but rich insights into user behaviors and perceptions).

::: {.notes}
**Explain Constructivism:** In this view, reality isn’t a single fixed thing to be measured – it’s what people perceive and experience.
- Multiple truths: One user might find an AI tool inspiring, another finds it frustrating – both realities are “true” for those individuals. A constructivist researcher wants to understand each perspective.
- The researcher often engages closely with participants – through conversations, observations in natural environments – and acknowledges their own influence. For instance, if I interview a designer about their workflow, my questions shape the conversation; I’m part of the knowledge creation.
- Methods are qualitative: instead of large-scale experiments, we might spend time with a small set of users, maybe shadow them or have open-ended interviews, to get depth.
- **Example expansion:** Imagine studying how teams incorporate an AI design assistant. A constructivist approach might involve visiting several design teams, observing meetings, interviewing members about their feelings and interpretations of the AI’s suggestions. The goal is understanding *why* and *how* people use or don’t use the AI, what it means to them, rather than declaring a single success metric.
:::

## Beyond the Basics: Other Paradigms

- **Post-positivism:** A tempered version of positivism. Accepts that absolute objectivity is impossible, yet still seeks objective knowledge with rigorous methods (uses statistics, probabilistic findings, etc.).
- **Critical Theory Paradigm:** Focuses on power, equity, and societal influences. Research is not just to understand or predict, but to critique and change society (e.g., feminist HCI studies, research on AI bias through a critical lens).
- **Pragmatism:** Very common in industry. Focuses on what works in practice – mixing methods to solve problems. Rather than committing to one philosophy, pragmatists pick the approach that best addresses the research question.
- **Why these matter:** They remind us that one’s worldview can expand research aims – e.g., a critical approach might shape research questions about the ethics of a design, while pragmatism encourages flexibility (often the case in product research teams).

::: {.notes}
**Notes:** We highlighted positivist and constructivist, but I want to acknowledge other paradigms:
- *Post-positivism:* Think of it as positivism with a bit of humility. Post-positivists still aim for objective truth but recognize research has biases and errors. They rely on stats to say "with 95% confidence, X is better than Y," acknowledging uncertainty.
- *Critical:* This paradigm is important if you are looking at design from a lens of social impact. For example, a critical researcher might study how an AI tool could reinforce biases or affect different user groups unequally. The goal can be to expose issues and advocate change, not just observe.
- *Pragmatic:* Most product teams are inherently pragmatic. The attitude is “use whatever method gives insight.” If user interviews, analytics, and A/B tests all help, a pragmatist does all of them. The focus is solving the problem, not philosophical consistency. 
- For you as future practitioners, pragmatism will likely resonate – but being aware of the other paradigms ensures you can justify *why* you're using certain methods and be mindful of their limitations.
:::

## **Activity 1 (10 min)** – Identify the Paradigm

**Scenario A:** A UX team sets up a **controlled experiment** to test a new app feature. 100 users get version X, 100 get version Y. They measure task completion time and error rates to see which version performs better.

**Scenario B:** A design researcher spends weeks **immersed in users’ environment** (office, home) observing how they use a prototype AI tool, taking notes on behaviors, and interviewing them about their experiences and feelings.

- **Your Task:** For each scenario, decide which research paradigm (positivist or constructivist) it most closely reflects. Why? Discuss briefly with a neighbor or think through the rationale.
- We’ll debrief in 10 minutes: what cues led you to your conclusions, and how the paradigm might influence the outcomes.

::: {.notes}
**Instructions for Activity 1:** 
- The goal is to get students recognizing hallmarks of paradigms in practice.
- Scenario A: clearly a positivist approach (controlled, quantitative, looking for generalizable better version).
- Scenario B: clearly constructivist (immersive, qualitative, context-rich).
- Ask students to pair up (or small groups) for ~5 minutes to decide for each scenario which paradigm it aligns with and one reason why. If remote or solo, they can jot down their reasoning.
- After ~5 minutes, bring the group together and ask: “Who thinks A is positivist? Why?” Highlight expected answers: (A) is positivist because it seeks objective measures and comparisons, researcher is hands-off apart from setup. “Who thinks B is constructivist? Why?” Answers: (B) is constructivist because it involves deep contextual understanding, no attempt to control variables, researcher is embedded with users.
- Emphasize there’s no trick here: A is positivist, B constructivist. Also note that in real projects we might combine elements of both – but the exercise is to clearly see the difference.
- Transition: Now that we can identify these paradigms, let’s talk about the actual methods (qualitative, quantitative) which often align with them.
:::

## Qualitative Research Methods

- **Focus:** Understanding “why?” and “how?” – capturing rich insights into user behaviors, motivations, and experiences ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=Qualitative%20studies%20are%20my%20most,behaviours%2C%20analyzing%20their%20lived%20experience)).
- **Approach:** In-depth engagement with relatively few individuals. Techniques include:
  - **Interviews (open-ended):** One-on-one or group (focus groups) discussions to hear users’ thoughts in their own words.
  - **Observations & Ethnography:** Watching users in context (at home, work) to see real behaviors and situational factors.
  - **Think-Aloud Usability Studies:** Users narrate their thought process while using a prototype, revealing pain points.
  - **Diary Studies:** Participants log their experiences over time, providing longitudinal insight.
- **Data & Outcomes:** Descriptive and narrative data (notes, recordings, quotes). Yields deep insights and new ideas, but not stats. Small sample sizes are typical – it’s about insight over numbers ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=solve%20with%20your%20product)).
- **Example:** Visiting 5 customers’ workplaces to observe how they currently solve a problem that a new product aims to address. The researcher notes workflows, frustrations, and workarounds, uncovering needs that weren’t obvious from surveys alone.

::: {.notes}
**Elaborate on Qualitative Methods:** 
- Qualitative = quality/depth of understanding. We engage directly with people. Unlike a multiple-choice survey, here participants tell stories, show us how they work, etc.
- Examples of methods:
  - Interviews: e.g., asking users “How do you currently accomplish task X? What are the biggest challenges?” It’s flexible, can probe interesting points on the fly.
  - Observations: e.g., shadow a nurse using a health app during their shift to see real usage and contexts (things they might not even mention in an interview).
  - Think-aloud: common in UX testing. We might sit a user down with our app prototype and ask them to voice what they’re thinking (“Now I want to find X… hmm where is that button?”) to catch usability issues and reasons behind actions.
  - Diary: useful for longitudinal insight. For example, get users to write daily for a week about how they use a new AI writing tool – we gather evolving impressions, contexts of use, etc.
- Emphasize that qualitative research doesn’t try to count how many people have issue Y – instead it tries to deeply understand what issues exist and why. So 5–10 people might be enough if they are providing repetitive insights. Nielsen’s classic UX mantra: “5 users can find 85% of usability problems” – that’s the idea of diminishing returns in qual.
- It's aligned often with interpretive paradigm: the researcher interprets the data (which can be subjective). We look for themes, patterns in narratives.
:::

## Quantitative Research Methods

- **Focus:** Understanding “how many?” or “how much?” – measuring phenomena in a way that can be quantified and generalized.
- **Approach:** Structured data collection from larger samples. Techniques include:
  - **Surveys & Questionnaires:** Standardized questions (often closed-ended) to capture frequencies, ratings, etc., from many respondents.
  - **Analytics & Log Analysis:** Using product data (click rates, time on task, error rates) to find usage patterns at scale.
  - **Experiments & A/B Tests:** Comparing outcomes under different conditions with statistical analysis to infer causality (e.g., does design variant A improve metric X over B?).
- **Data & Outcomes:** Numerical data, statistics, charts. Goal is generalizable findings (e.g., “X% of users experienced Y”). Requires larger sample sizes for confidence – e.g., ~20+ users per group for usability metrics to be reliable ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=Sample%20size%20for%20quantitative%20researches,design%20option%20with%2020%2B%20users)).
- **Example:** Launching two versions of a feature to 1,000 users each and measuring that 65% of users complete the task with version A vs. 50% with version B – a statistically significant improvement. This quantitative result provides evidence of one design’s superiority on that metric ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=Quantitative%20methods%20are%20about%20numbers,answers%20to%E2%80%9Chow%20many%2Fhow%20much%E2%80%9D%20questions)) ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=,)).

::: {.notes}
**Elaborate on Quantitative Methods:** 
- Quantitative = quantity/numbers. We turn observations into data that can be counted or measured.
- Common methods:
  - Surveys: e.g., send out a questionnaire to 500 users asking them to rate satisfaction on a 1-5 scale. We can calculate an average or see what percentage are satisfied. Surveys are good for broad reach.
  - Analytics: if a product is live, tools like Google Analytics or in-app telemetry can tell us “5000 users clicked that button, average time on page is 30 seconds, conversion rate is 15%,” etc. This is passive but large-scale.
  - Controlled experiments (A/B tests or multivariate): e.g., show different users slightly different designs or algorithms and see which performs better on predefined metrics (click-through, error rate, etc.). This is gold standard for causality in product settings.
- Key outcome: statistics. We might use measures of central tendency, dispersion, significance tests. The idea is to generalize – not just these 1000 users, but likely all users have similar behavior (if sample is representative).
- Emphasize sample size: In quant, “N” (sample size) matters. A rule of thumb from Nielsen Norman Group is at least 20 users per condition for comparing designs, to have enough data ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=Sample%20size%20for%20quantitative%20researches,design%20option%20with%2020%2B%20users)). With smaller N, differences might be just noise.
- **Example detail:** The slide’s example describes an A/B test. If version A’s completion rate is 65% and version B’s is 50%, that difference (15 percentage points) can be tested for statistical significance. If significant, we conclude version A likely truly is better for the population. This kind of evidence is compelling to data-driven stakeholders.
:::

## Mixed Methods & Triangulation

- **Combining Qual & Quant:** Often the best insights come from mixing approaches. Qualitative and quantitative methods can complement each other ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=We%20may%20decide%20to%20do,be%20causing%20issues%20we%E2%80%99ve%20observed)):
  - Use **qualitative** research to explore new problem spaces, generate hypotheses or design ideas, then **quantitative** research to test those ideas at scale.
  - Or vice versa: use **quantitative** data to identify an issue or pattern, then **qualitative** inquiry to find out *why* that pattern occurs.
- **Triangulation:** Looking at a question from multiple angles increases confidence. If different methods (interviews, surveys, analytics) all point to the same conclusion, it's likely robust.
- **Pragmatic Approach:** In design research, it’s common to be method-agnostic – choose the method that fits each stage of the project (a pragmatic paradigm mindset).
- **Tools & Platforms:** Modern UX research platforms let us do mixed methods efficiently. *Example:* Using a tool that records users’ sessions on a Figma prototype – we can collect quantitative metrics (task completion rates, click counts) and qualitative observations (where users got confused) at the same time ([Figma Prototype Testing: How to Test With Users | UXtweak](https://blog.uxtweak.com/how-to-test-figma-prototypes/#:~:text=Figma%20Prototype%20Testing%3A%20How%20to,what%20works%20and%20what%20doesn%27t)).
- **Example Workflow:** After launching a beta, the team sees in analytics that only 30% of users use a new AI feature (quantitative finding). They then interview 10 users (qualitative) and discover common reasons: the feature is hard to find and users aren’t sure of its benefits. This combination pinpoints both the *what* and the *why*.

::: {.notes}
**Discuss Mixed Methods:** 
- Emphasize that qual vs quant is not an either/or choice; they answer different questions. Combining them gives a fuller picture.
- For instance, a qualitative study might suggest users feel overwhelmed by an interface – then a quantitative study (survey) could tell you 70% of users report feeling stressed (confirming it's widespread). Or a quant study might show users drop off at step 3 of a funnel – then qual usability sessions reveal what about step 3 is problematic (maybe confusing wording).
- This iterative dance is common: start with qual to uncover issues, then quantify how prevalent or impactful they are; or start with quant data, then use qual to diagnose issues.
- **Triangulation:** It’s an academic term meaning verifying a result with multiple methods or data sources. For example, if usage logs, survey responses, and interview quotes all indicate the AI feature is too slow, you can be very confident that's a real issue.
- **Tools example detail:** Mention that now there are usability analytics tools (like Maze, UserTesting, etc.) that integrate with Figma prototypes. They let you track things like where people clicked (quant) and also record video or open feedback (qual).  ([Figma Prototype Testing: How to Test With Users | UXtweak](https://blog.uxtweak.com/how-to-test-figma-prototypes/#:~:text=Figma%20Prototype%20Testing%3A%20How%20to,what%20works%20and%20what%20doesn%27t)) highlights that Figma prototyping can yield both quantitative and qualitative feedback in early stages.
- Many companies adopt a **pragmatic** research cycle often called **“Discovery and Validation”**: 
  - Discovery (generative research) is typically qualitative (e.g., user interviews to discover needs).
  - Validation (evaluative research) might be quantitative (e.g., A/B test to validate which solution works better).
  - This way, you benefit from both exploration and measurement.
:::

## Case Study: Combining Methods in Action

- **The Problem:** An e-commerce team finds conversion rates are lower than expected in the checkout flow of their app (many users abandoning before purchase).
- **Quantitative Clue:** Funnel analytics show a big drop-off between the payment step and the address step in checkout – most users who reach payment don’t finish to the address page ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=2,users%20from%20payment%20to%20address)).
- **Qualitative Insight:** Researchers then replay user session recordings (or conduct usability tests) for the checkout process. They observe many users quitting on the payment page. Findings: the payment form was long and not mobile-friendly, and the “Next” button was hidden below the fold on small screens ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=4,validation%20is%20hidden%20out%20of)).
- **Outcome:** By triangulating data and observation, the team identifies *why* users drop off. They redesign the payment page (shorter form, more visible button) and subsequently see conversion improve. 
- **Key Point:** The data (quant) pointed **to where** the issue was, and the user sessions (qual) explained **why** it was happening – a clear example of mixed methods solving a real design problem.

::: {.notes}
**Walk through the case:** This example illustrates how a practical product team might use both data analysis and user-focused observation:
- The **quantitative part** is the funnel metrics. Many analytics dashboards will show something like “50% of users who started checkout never completed.” Here they narrowed it down: a specific step (payment to address) had the biggest drop. That’s a red flag – quant tells us *where* to look.
- But quant alone doesn’t tell **why** users drop at payment. Maybe the price is too high? Maybe a bug? So the team uses a qualitative method: session replay (using a tool like FullStory or Hotjar, etc.) or direct user testing.
- The **qualitative findings** are concrete: users scroll, look at payment options, then leave; the form is too long on mobile; the Next button isn’t immediately visible so users think maybe they’re done or get frustrated.
- By combining, they got a full story. The fix becomes obvious (UX redesign of that page).
- Emphasize to students: This is a common pattern in product research – analytics pinpoint problems at scale, then qualitative dives provide design fixes. If they only did one or the other, they might miss the full picture (e.g., if only did user tests, they might find issues but not realize how many users it affects; if only did analytics, they’d see a drop-off but might misguess the cause).
:::

## Design-Based Research (DBR)

- **What is DBR?** A methodology that integrates **design** and **research** in iterative cycles. Researchers create an **intervention** (e.g., a new tool, prototype, or educational program) to address a problem, and then study it in real use to see how well it works and why ([Design-based research - Wikipedia](https://en.wikipedia.org/wiki/Design-based_research#:~:text=Design,test%20how%20well%20they%20work)).
- **Key Features:** 
  - *Iterative Cycles:* Design a solution, implement it, gather data on its performance, then refine the solution and repeat. Learning occurs with each cycle.
  - *Real-world Context:* Unlike a lab experiment, DBR happens in natural settings (classrooms, workplaces, real user environments). The context and participants co-shape the outcomes.
  - *Blend of Methods:* Often mixes qualitative and quantitative data – e.g., observations, user feedback, and usage metrics across iterations.
  - *Goal:* Both **practical improvement** of the design and **theoretical knowledge**. DBR aims to generate general insights or design principles from the process, not just a one-off fix.
- **Examples:** 
  - In education tech (where DBR originated), a team might design a new learning app, pilot it in a class, gather feedback and performance data, then tweak the app and test again, eventually publishing both a better app and insights on learning.
  - In HCI/product design, think of developing a novel UI through iterative prototyping with users and using each iteration’s findings to inform design theory (this is akin to “Research through Design” in HCI).

::: {.notes}
**Further explain Design-Based Research:** 
- DBR comes from fields like the Learning Sciences. Instead of isolating one variable in a lab, it embraces the complexity of real use. It's pragmatic and constructivist in spirit – knowledge emerges from trying to solve a problem in context.
- **Process:** Typically, DBR is framed as: 
  1. Identify a practical problem.
  2. Design an intervention (maybe based on theory or prior research).
  3. Implement it in a real setting.
  4. Collect data on what happens (could be test scores, user engagement, observational notes).
  5. Analyze and refine both the design and the underlying theory or assumptions.
  6. Repeat in further iterations or different contexts.
- Contrast with a traditional experiment: DBR doesn’t have a control group in a lab. It’s more like “let's improve this thing and learn along the way.” Critics (like some psychologists) say it’s not controlled enough ([Design-based research - Wikipedia](https://en.wikipedia.org/wiki/Design-based_research#:~:text=Design,design%20experiments%20%27neither%20designed%2C%20nor)), but proponents argue it’s the best way to tackle complex, real-world problems that can’t be distilled into a single variable.
- **Research-through-Design (RtD):** In design fields, a similar concept is RtD – where designing an artifact is a way to generate knowledge. For instance, a designer might create a series of experimental prototypes (maybe unconventional UIs) and document what they learn about user interaction from them. It’s often constructivist: knowledge is contextual, but still valuable.
- **Why for us?** If you’re innovating with new tech (say designing a radically new AI interface), DBR/RtD approach is useful. You build a prototype, test it with users (maybe in context for a while, not just one-off), learn from how they use or misuse it, improve it, and gradually formalize some principles (like “users need X to trust the AI, as evidenced by our iterations”).
- Emphasize dual goal: solving the immediate design problem and contributing to broader understanding (which could be shared via a case study or internal best practices).
:::

## Academic vs. Industry Research Mindsets

- **Academic Research (Traditional):** Emphasizes rigor, control, and theory:
  - Prioritizes **reliability, validity, and rigor** in methods and analysis ([Research through Design | The Encyclopedia of Human-Computer Interaction, 2nd Ed.](https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/research-through-design?srsltid=AfmBOoqxpia4uY8oAqZEKjpnxj32blW79Jg1arlcz8fbcfyNf6Oc_exi#:~:text=Has%20borrowed%20heavily%20from%20the,research%3A%20reliability%2C%20validity%2C%20and%20rigor)).
  - Often seeks generalizable knowledge or new theory; results typically published for peer review.
  - Timeline can be longer; depth often valued over speed.
  - Example: A university HCI lab might run a months-long study on how people use AR interfaces, with formal hypotheses and statistical validation.
- **Industry/Design Research:** Emphasizes relevance, speed, and actionable insights:
  - Values **relevance, creativity, and practical impact** (generativity, evocativeness of ideas) in research ([Research through Design | The Encyclopedia of Human-Computer Interaction, 2nd Ed.](https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/research-through-design?srsltid=AfmBOoqxpia4uY8oAqZEKjpnxj32blW79Jg1arlcz8fbcfyNf6Oc_exi#:~:text=Has%20borrowed%20heavily%20from%20the,research%3A%20reliability%2C%20validity%2C%20and%20rigor)).
  - Goal is to inform design decisions quickly and effectively; "good enough" data to move forward is often sufficient.
  - Often mixes methods pragmatically (as needed to meet product goals), and results are used internally rather than broadly generalized.
  - Example: A startup conducts a quick 1-week design sprint with user interviews and prototype tests to decide on a feature direction – fast and focused on the product at hand.
- **Bridging the Two:** Top-tier design research tries to balance these. We want solid evidence (not just gut feeling), but also need to deliver on project timelines. Knowing academic principles helps avoid biases and sloppy data, while an industry mindset keeps things user-centered and outcome-oriented.

::: {.notes}
**Discuss differences and balance:** 
- This slide highlights that there can be a tension between how research is done in academia vs in product teams.
- Academic research:
  - Terms like reliability and validity are paramount. (Reliability = would we get the same results if repeated? Validity = are we truly measuring what we think we are?)
  - There’s a strong culture of thorough documentation, literature review, and peer scrutiny. That’s why academia often yields very trustworthy findings but can be slow and narrow in scope.
- Industry research:
  - The concept of “relevance” means results have to matter to the current design problem. A super rigorous study that doesn’t impact a design decision is seen as a waste in industry.
  - Also, being generative/evocative – often design research in practice tries to inspire the team (e.g., through personas, user journey stories) in addition to providing data.
  - Speed matters – time is money. So methods might be more lean (e.g., talk to 5 users tomorrow rather than recruit 50 for a formal study).
- Sanders (2005) described these differences ([Research through Design | The Encyclopedia of Human-Computer Interaction, 2nd Ed.](https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/research-through-design?srsltid=AfmBOoqxpia4uY8oAqZEKjpnxj32blW79Jg1arlcz8fbcfyNf6Oc_exi#:~:text=Has%20borrowed%20heavily%20from%20the,research%3A%20reliability%2C%20validity%2C%20and%20rigor)): academic-style = more like social science (structured, past-focused), designerly = more future-oriented and creative.
- Both approaches have value. If too academic, design may miss the market window or over-analyze; if too industry (fast & loose), you risk biases, false conclusions or missing underlying issues.
- For you as aspiring PMs or UX leaders: the trick is to inject academic rigor *just enough* to ensure quality (for instance, avoid leading questions in user interviews, use solid metrics for A/B tests) while maintaining the agility of design practice (not every study needs to be published, just useful).
:::

## Researching User Experience with Generative AI

- **New Challenges:** Generative AI systems (like GPT-based tools or image generators) can behave unpredictably and interact with users in complex ways. Evaluating them requires looking beyond traditional usability. Key questions include: Do users **trust** the AI? How do they understand its suggestions or outputs? Are there misuse or ethical concerns?
- **Start Early (even before the AI is built):** You can and should get user feedback **before** a model is fully functional ([User Research for Machine Learning Systems: A Case Study Walkthrough](https://dscout.com/people-nerds/user-research-for-machine-learning#:~:text=Can%20you%20do%20UX%20Research,ready%20model)). Techniques like **Wizard-of-Oz prototyping** allow simulating AI behavior (e.g., a human pretending to be the AI) to test concepts. Early qualitative research (interviews, concept tests) reveals user expectations and mental models for the AI.
- **Combine Data Science & UX:** Developing AI products benefits from a hybrid research approach. One case study suggests using data science techniques (to analyze system behavior or user data) alongside UX tactics to make AI systems more **explainable and controllable** for users ([User Research for Machine Learning Systems: A Case Study Walkthrough](https://dscout.com/people-nerds/user-research-for-machine-learning#:~:text=This%20case%20study%20outlines%20my,can%20better%20control%20their%20experience)). In practice, this means:
  - Analyze model output data to identify patterns or errors (quantitative).
  - Also conduct user studies to see how people react to those outputs and whether they understand them (qualitative).
- **Example – AI Design Assistant:** Imagine testing a Figma plugin powered by GPT that suggests design improvements. You’d measure **usage data** (e.g., how often do designers accept suggestions? which suggestions are ignored?) *and* gather **feedback** (do designers feel it boosts creativity or do they find it annoying?). This mixed approach uncovers both performance metrics and user sentiment.
- **Focus on Trust & Ethics:** Because GenAI can produce wrong or biased results, user research should probe trust (e.g., do users verify AI outputs?), transparency (do they understand why the AI suggested something?), and broader impacts. Methods like think-aloud usability tests, longitudinal studies (observing usage over time), and even surveys on user comfort can be vital to ensure the AI feature is adopted and used responsibly.

::: {.notes}
**Discussion:** Generative AI is a hot and evolving area, and researching UX for it has some unique aspects:
- Traditional UX metrics (like task time, error rate) might not capture the whole picture. For AI, *user trust* and *mental model* are huge: Does the user understand what the AI is doing? Do they trust it enough to follow its suggestions? 
- **Wizard-of-Oz:** Explain this if students aren’t familiar: It’s when you fake an AI or complex system behind the scenes with a human. For example, before building a complex AI, you might have a researcher manually generate some suggestions or responses and present them as if from an AI. Users interact normally, and you observe. This lets you test the concept cheaply. The citation ([User Research for Machine Learning Systems: A Case Study Walkthrough](https://dscout.com/people-nerds/user-research-for-machine-learning#:~:text=Can%20you%20do%20UX%20Research,ready%20model)) reinforces the idea that yes, you can test ML systems with users early.
- **Data + UX combo:** AI systems produce a lot of data (confidence scores, output logs, etc.) which data scientists analyze. But you also need the human perspective (maybe the AI is accurate but users hate the tone of its suggestions). The quote ([User Research for Machine Learning Systems: A Case Study Walkthrough](https://dscout.com/people-nerds/user-research-for-machine-learning#:~:text=This%20case%20study%20outlines%20my,can%20better%20control%20their%20experience)) basically says: use ML techniques for data-driven decisions and UX methods to ensure the system is understandable and controllable for users. 
  - For instance, if an AI sorts content, a data scientist might cluster usage logs to see patterns (unsupervised learning) while a UX researcher might do a study to see if users can figure out how to influence the AI’s output.
- **Example (AI Design Assistant):** This makes it concrete for our audience. Suppose Figma had an AI that recommends improvements (like “hey, your font sizes are inconsistent, want to fix?”). We’d want to track usage (maybe only 20% accept suggestions - why so low?). Then we’d interview or survey: maybe they feel the suggestions aren’t trustworthy or disrupt their flow.
- **Trust & Ethics:** Encourage students to think beyond functionality. If an AI tool occasionally gives a wrong answer, how do users detect that? Do they over-rely on it? Are any biases creeping in (maybe the AI suggests designs that inadvertently reflect a bias)? Research should include questions about these. Techniques can include:
  - Ask users to explain what they think the AI is doing (to gauge their understanding).
  - Scenarios to see if users notice when the AI is wrong.
  - Include a diverse user group to see different perspectives (maybe novices vs experts trust differently).
  - If it's a long-term tool, maybe a diary study to see how trust evolves over a month of use.
- In summary, researching GenAI UX is inherently interdisciplinary: you need to evaluate the algorithm’s performance *and* the human’s experience together.
:::

## **Activity 2 (15–20 min)** – Plan a Research Strategy for a GenAI Feature

**Scenario:** You are the product team for a new *AI-powered plugin in Figma* that automatically suggests design improvements (layout tweaks, color suggestions, accessibility fixes) to designers as they work.

- **Goal:** Develop a research plan to guide the design and refinement of this AI feature.
- **In Teams:** Spend the next ~15 minutes outlining:
  1. **Key research questions:** What do you need to learn? (e.g., Do designers trust the suggestions? Does it actually speed up design work? What problems or biases might occur?)
  2. **Methods to use:** Choose at least two methods (mix qualitative/quantitative). When and how will you use them? 
     - *Generative/exploratory phase:* e.g., contextual inquiry or interviews with designers to understand how they might use such a tool and what they need.
     - *Prototyping/testing phase:* e.g., create a prototype of the plugin (even a Wizard-of-Oz simulation in Figma) and conduct usability tests or A/B comparisons.
     - *Post-launch/iterative phase:* e.g., instrumentation to collect usage analytics, plus follow-up surveys for user satisfaction.
  3. **Use of Figma:** How will you leverage Figma in the research? Perhaps to build interactive prototype scenarios for testing, or to collect feedback directly in the design context.
- **Deliverable:** Be ready to briefly share your plan: which methods you chose and why, and how they complement each other to ensure both the AI’s performance and the UX are evaluated.
- *(If working solo, sketch your plan and consider writing a short research plan outline.)*

::: {.notes}
**Guidance for Activity 2:** 
- This is a capstone activity pulling everything together. Students should apply paradigm thinking and method knowledge to a realistic project.
- **Team setup:** Form small groups (3-5 students ideally). If remote/asynchronous, individuals can do it but encourage sharing afterward.
- **Clarify scenario:** An AI plugin in Figma suggesting design improvements. Essentially an “auto-designer assistant.” It’s a generative AI use-case in a UI context they’re familiar with.
- They need to think of what to research:
  - Perhaps start with: “What would success look like for this feature? And what might go wrong?” That yields research questions: e.g., Will designers use or ignore the suggestions? Do suggestions actually help or annoy? Are there types of suggestions that are always rejected? Do users trust the AI on important decisions (colors, spacing)? Are there any failure cases (bad suggestions that could embarrass the designer)? User needs/expectations?
- **Encourage mixing methods:** For example:
  - Early on, interviews or observations of designers working *without* the AI to see where it could fit (this identifies needs).
  - Then a prototype test: maybe design two prototype versions (one with subtle suggestions, one with aggressive auto-changes) and do a usability study or A/B test in a workshop.
  - After an initial version, gather analytics: measure how often suggestions are accepted, measure time saved, etc. And perhaps a survey or interviews after a week of use to get subjective feedback (do they feel it improved their workflow?).
  - Even a diary study could be proposed if someone uses it over a week on a real project, to see how their attitude changes.
- **Figma’s role:** They can actually create a fake plugin UI in Figma (like a sidebar that lists suggestions) as a prototype for the test. Or use Figma’s prototyping to simulate the AI suggestions popping up.
  - Also, Figma could allow instrumenting the prototype to see what people click (with tools like Maze).
- **Time check:** 15 minutes is short. Suggest to them to outline bullets rather than write a full plan. It’s about the reasoning: what methods and why.
- **After 15 min:** Have each group (or a few groups, depending on time) share highlights. Possibly ask: “Which methods did you choose for exploratory vs evaluative? Why?” or “How will you know if your AI plugin is successful according to your research plan?” 
- Use their sharing to highlight good mixes: e.g., “Group A will do interviews then usage analytics – great mix of qual and quant. Group B wants to run a wizard-of-oz test with a fake AI – that’s a fantastic idea to get early feedback,” etc.
- Connect back to lecture content explicitly: They are basically doing what we learned – identifying questions, choosing qual/quant, considering iterative design (DBR style), etc., and even ethics (maybe someone will mention checking if AI suggestions are biased or not useful for certain styles).
:::

## Key Takeaways

- **Multiple Paradigms, Multiple Insights:** There’s no one “right” way to investigate design questions. Recognize the lens (positivist, constructivist, etc.) you’re using – it influences what you see. Often, a **pragmatic mix** works best in product settings (use scientific rigor when needed, but stay user-centered and context-aware).
- **Method Fit > Method Fashion:** Choose research methods based on the questions at hand, not just trends. Each method has strengths/limits – **triangulate** findings via both qualitative and quantitative approaches for confidence.
- **Iterate and Integrate:** Borrow from design practices – iterate your research just like design iterations. Early exploratory studies inform initial designs; later evaluative studies test and refine them. In emerging tech, this cycle is crucial as understanding evolves.
- **Users at the Heart:** Whether measuring numbers or collecting stories, remember the goal is to improve user experience. Keep interpreting what the data means for real users. For GenAI and new tech, pay extra attention to user trust, understanding, and ethical impact.
- **Research Design Matters:** *“The design of your research might be more important than the implementation… We need to find the best methods to prove or disprove our hypotheses.”* ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=The%20design%20of%20your%20research,than%20the%20implementation%20of%20it)) A well-thought-out research plan (with clear questions and appropriate methods) will yield actionable knowledge, whereas a sloppy plan with fancy tools won’t help. Invest time in planning how to ask and answer the right questions.
- **Continuous Learning:** As aspiring design leaders and data scientists, build your “research mindset.” Even when not formally a researcher, approach problems with curiosity, gather evidence, and be ready to learn and adapt. That is how you’ll navigate the unknowns of designing innovative products.

::: {.notes}
**Wrap-up notes:** 
- Reiterate that understanding research methodologies is empowering. It makes one a better problem solver in tech/design.
- Summarize each bullet:
  - Paradigms: Encourage them to be aware of biases. If someone says “users are just numbers” vs “users are unique snowflakes,” those are paradigmatic leanings. Balance objective data with empathy.
  - Method fit: Don’t do an ethnography just because it sounds cool, and don’t run an A/B test without a hypothesis. Always ask: what am I trying to learn, and what method will get me that insight efficiently?
  - Iterate: Just like design, research can be agile. You might do a quick study, implement changes, then another study. It’s not one-and-done, especially with new tech where you uncover new questions along the way (like our DBR discussion).
  - Users at heart: Keep tying research findings back to user experience improvements. Data is a means to empathy, not an end. For AI, also ensure we consider human values (don’t just optimize a metric and ignore that users might feel creeped out, for example).
  - Research design matters: The quote from the UX Collective piece ([Qualitative and Quantitative Studies in Product Design: Concepts, Examples, and a Rabbit Hole | UX Collective](https://uxdesign.cc/qualitative-and-quantitative-studies-concepts-examples-and-a-rabbit-hole-c44482f5b969#:~:text=The%20design%20of%20your%20research,than%20the%20implementation%20of%20it)) drives home that planning (which methods, in what sequence) is crucial. I often say “a problem well-stated is half-solved” – similarly a research question well-posed is half-answered because it guides method choice clearly.
  - Continuous learning: Encourage them to practice these skills. In their projects or even daily work, be the person who asks “how do we know?” and suggests an evidence-gathering approach. That’s the crux of being research-minded.
- Thank the audience, and open for any final questions or discussion. If relevant, mention any resources or references (perhaps the ones cited throughout, or textbooks like Creswell on research design, or Nielsen Norman Group articles for UX methods).
:::

