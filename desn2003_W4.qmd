---
title: "DESN2003: Research for Innovation"
subtitle: "Week 4: Research Questions (From Gap to Inquiry)"
author: "Dr. Hongshan Guo"
format:
  revealjs:
    slide-number: true
    theme: simple
    width: 1600
    height: 1200
    embed-resources: true
    incremental: true
    center: true
    footer: "DESN2003 Research for Innovation"
---

# Quick — How Did Last Week Go? {background-color="#A8DADC"}

## You Went Landscape Hunting in Week 3

Reddit threads. App Store 1-star reviews. Google Trends. Wayback Machine. Maybe even some Google Scholar.

. . .

**Be honest — how do you feel right now?**

::: {.incremental}
- "I found too much stuff and I don't know what matters"
- "I found almost nothing — is my topic even real?"
- "I found interesting things but I don't know what to do with them"
- "I feel like I'm going in circles"
:::

. . .

**All of these are normal.** And all of them point to the same problem.

---

## The Problem: Landscape Without a Compass

::: {.columns}
::: {.column width="50%"}
**What Week 3 gave you:**

- A pile of signals
- Some competitor info
- Maybe a few papers
- A general sense of "there's something here"

This is **good raw material.**
:::

::: {.column width="50%"}
**What Week 3 didn't give you:**

- A way to decide what's relevant vs. noise
- A targeted search strategy for Doc 0.1
- A filter that tells you "read this, skip that"
- A clear next step

This is **today's job.**
:::
:::

. . .

::: {.callout-important icon=false}
## Today in One Sentence

**Last week you learned how to find the landscape. This week you build the compass — a research question that tells you exactly what to search for next, and what to ignore.**
:::

---

## Why This Matters Right Now

::: {.nonincremental}
Your **Doc 0.1 (Literature Review)** is due Week 6.

That means in two weeks you need to do a *targeted* literature search — not another broad scan.

**Without a sharp research question**, your lit review will be:

- "Here are 10 papers I found" (a reading list, not a review)

**With a sharp research question**, your lit review will be:

- "Here's what's known about [my specific question], here's where the evidence is thin, and here's what I'll investigate" (actual scholarship)
:::

The research question is the bridge between Week 3 (find stuff) and Doc 0.1 (make sense of it).

---

## The Shape of Today

::: {.columns}
::: {.column width="50%"}
**First half — Build the skill**

1. What separates a topic from a question?
2. What makes a question good or bad?
3. How do you scope it right?
4. Live rating: judge real questions together

*You'll develop the instinct.*
:::

::: {.column width="50%"}
**Second half — Apply it to yours**

5. Deepen your W3 signals with real sources
6. Draft your own research question
7. Peer feedback: stress-test it
8. Leave with a working compass for Doc 0.1

*You'll leave with something concrete.*
:::
:::

---

## The Toolkit You're Building This Semester

Every few weeks, you'll pick up a new research tool. By the end you'll have a complete kit you can carry into capstones, theses, internships — any project where you need to go from "I have a hunch" to "here's the evidence."

::: {.nonincremental}
| Week | Tool | What It Does | Status |
|:----:|------|-------------|:------:|
| 3 | [Landscape Intelligence Canvas](data/landscape_canvas.pdf) | Map competitors + academic landscape | Done |
| **4** | **[Research Question Builder](data/research_question_builder.pdf)** | **Sharpen your gap into a searchable question** | **Today** |
| 5 | [Interview Protocol Template](data/interview_protocol_template.pdf) | Structure qualitative interviews | Coming |
| 5 | [Worksheet: Qualitative Methods](data/worksheet.pdf) | Quick-reference for interview best practices | Coming |
| 6 | [Survey Design Checklist](data/survey_design_checklist.pdf) | Design surveys that avoid common bias traps | Coming |
| 9 | [Synthesis Canvas](data/synthesis_canvas.pdf) | Combine qual + quant findings into insight | Coming |
| 9 | [Research to Decision Canvas](data/research_to_decision_canvas.pdf) | Turn research into product recommendations | Coming |
:::

. . .

::: {.callout-tip icon=false}
## These Are Yours to Keep
Each PDF has a blank template (page 1) and a worked example (page 2). They're designed to be reusable — print them, fill them digitally, bring them to other courses. The toolkit page on the course site has all of them in one place.
:::

---

## Two Students Walk Into Office Hours...

::: {.columns}
::: {.column width="50%"}
**Student A:**

*"I want to research sustainability in design."*

. . .

- What about sustainability?
- What about design?
- What would you even measure?
- Where would you start?

**This is a topic. It's not searchable.**

You'd type it into Google Scholar and get 2 million results.
:::

::: {.column width="50%"}
**Student B:**

*"I want to understand how material choices in packaging affect Hong Kong consumers' perception of product sustainability."*

. . .

- Specific phenomenon (material choices)
- Defined population (HK consumers)
- Clear outcome (perception)
- You can imagine the search terms already

**This is a research question. It's a filter.**

Google Scholar goes from 2 million → 200 → the 15 that matter.
:::
:::

---

## Learning Objectives

By the end of this session, you will be able to:

::: {.incremental}
1. **Distinguish** between a topic, a gap, and a research question — and know why it matters for your literature search
2. **Formulate** a focused research question from the landscape work you did in Week 3
3. **Evaluate** whether a question is answerable, searchable, and appropriately scoped
4. **Leave with** a working research question that gives you a targeted search strategy for Doc 0.1
:::

# From Topic to Question {background-color="#A8DADC"}

## Doc 0: You Just Submitted — Quick Reality Check

::: {.callout-note icon=false}
## Feedback is coming within the week.

But here's a preview: the most common pattern in Doc 0 submissions is **describing an area** instead of **asking a question.** Today fixes that.
:::

. . .

**Three things that look similar but are very different:**

::: {.incremental}
- **Topic** — "I'm interested in sustainability and design" → *2 million Google Scholar results. Now what?*
- **Gap** — "Nobody has studied how material choices affect perception in HK" → *Closer, but still not searchable*
- **Research Question** — "How do material choices in packaging affect HK consumers' perception of product sustainability?" → *Now you know exactly what to type into Scholar*
:::

---

## See the Difference? It's a Zoom Lens.

::: {.columns}
::: {.column width="33%"}
### Topic
*Wide angle — everything in frame*

"Social media and mental health"

- Captures the whole landscape
- But you can't focus on anything
- **Search result:** 2,000,000+
:::

::: {.column width="33%"}
### Gap
*Medium shot — area of interest*

"Little research on design interventions for Instagram-related anxiety"

- Shows what's missing
- But what exactly will *you* study?
- **Search result:** ~5,000
:::

::: {.column width="33%"}
### Research Question
*Close-up — your specific frame*

"How do intentional friction features (like scroll limits) affect reported anxiety levels in young adult Instagram users?"

- You know the who, what, and how
- You can imagine the study
- **Search result:** 50–200 that actually matter
:::
:::

. . .

::: {.callout-tip icon=false}
## The Zoom Lens Test
If your question returns millions of results, zoom in. If it returns almost nothing, you might be too narrow — or you've found a genuine gap. Either way, **the question controls the search.**
:::

# What Makes a Research Question? {background-color="#FFE66D"}

## Anatomy of a Good Research Question

A strong research question is:

::: {.columns}
::: {.column width="50%"}
**FINER Criteria**

- **F**easible — Can you actually do this?
- **I**nteresting — Do you (and others) care?
- **N**ovel — Does it add something new?
- **E**thical — Can it be done responsibly?
- **R**elevant — Does it matter to the field?
:::

::: {.column width="50%"}
**Practical Checks**

- Can you answer it in one semester?
- Do you have access to the data/people needed?
- Is it specific enough to guide your methods?
- Is it open enough to allow discovery?
:::
:::

. . .

::: {.callout-important icon=false}
## The Innovation Check

**Also ask:** Does answering this question lead to **incremental improvement** or **genuine innovation**?

- Incremental: "Which color button works better?" (optimization)
- Innovation: "Why do users abandon this entire flow?" (discovery)

Both are valid — but know which you're doing.
:::

---

## Good vs. Bad Research Questions

::: {.columns}
::: {.column width="50%"}
### Weak Questions

- "What is sustainable design?"
  - *Too broad, definitional*

- "Is Instagram bad for mental health?"
  - *Too binary, too broad*

- "Why don't people recycle?"
  - *Too vague, no focus*

- "Will my app be successful?"
  - *Not researchable*
:::

::: {.column width="50%"}
### Strong Questions

- "How do material choices affect user perception of sustainability in product packaging?"
  - *Specific, testable*

- "What design features correlate with reduced self-reported anxiety in social media apps?"
  - *Focused, measurable*

- "What barriers do HK residents cite for recycling participation?"
  - *Bounded, answerable*

- "How do users perceive the usefulness of [specific feature] in [context]?"
  - *Evaluable*
:::
:::

---

## Real Papers, Real Questions: What Gets Published?

Let's look at actual research questions from published papers:

::: {.callout-note icon=false}
## Example 1: Urban Design + User Behavior

**Paper:** [Influence of Rules on User Behavior in Public Open Space of Hong Kong](http://article.sapub.org/10.5923.j.arch.20201004.02.html)

**RQ:** "What is the relation between explicit rules and observed behaviors in Hong Kong's public open spaces?"

*Why it works:* Specific context (HK), clear variables (rules vs. behaviors), observable outcomes
:::

---

## Real Papers, Real Questions (Cont'd)

::: {.callout-note icon=false}
## Example 2: AI + Cultural Heritage

**Paper:** [Artificial Intelligence for Dunhuang Cultural Heritage Protection](https://link.springer.com/article/10.1007/s11263-022-01665-x) (International Journal of Computer Vision)

**RQ:** "Can deep learning networks automatically restore deteriorated Dunhuang murals to quality comparable with manual restoration by archaeologists?"

*Why it works:* Specific artifact (Dunhuang murals), clear method (deep learning), measurable outcome (comparable quality)
:::

. . .

::: {.callout-note icon=false}
## Example 3: Sustainable Materials + Perception

**Paper:** [Sustainable materials: a linking bridge between material perception, affordance, and aesthetics](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1307467/full)

**RQ:** "How does material perception influence sustainable behavior decisions and design choices?"

*Why it works:* Connects perception to behavior, interdisciplinary (psychology + design), practical implications
:::

---

## What Do These Have in Common?

::: {.incremental}
1. **Specific context** — not "public space" but "HK public open space"
2. **Clear variables** — what's being examined, what's being measured
3. **Bounded scope** — answerable within the study's constraints
4. **So-what factor** — why should anyone care about the answer?
5. **Method-ready** — you can imagine how to study it
:::

. . .

::: {.callout-tip icon=false}
## The Publication Test
If a reviewer can't immediately see how you'd answer the question, it's too vague. If they think "so what?", it's too trivial.
:::

---

## The Question Formulation Framework

**Start with your gap, then build the question:**

::: {.nonincremental}
1. **Who** is the population? (e.g., "young adult Instagram users")
2. **What** is the phenomenon? (e.g., "intentional friction features")
3. **Where** is the context? (e.g., "in daily social media use")
4. **When** is the timeframe? (e.g., "over a two-week period")
5. **How/What** is the inquiry? (e.g., "affect reported anxiety levels")
:::

. . .

**Template:**

> "How does [phenomenon] affect/influence/relate to [outcome] among [population] in [context]?"

> "What [factors/features/experiences] contribute to [outcome] for [population]?"

---

## Diverse Research is Valid Research

Your question doesn't have to be about apps or products. Design research spans:

::: {.columns}
::: {.column width="50%"}
**Product/UX:**
- User behavior in digital interfaces
- Feature adoption and perception
- Interaction patterns

**Urban/Architecture:**
- [Public space usage in HK](https://cityterritoryarchitecture.springeropen.com/articles/10.1186/s40410-023-00194-5)
- Pedestrian behavior and wayfinding
- Built environment and wellbeing
:::

::: {.column width="50%"}
**Computational/Cultural:**
- [AI for heritage restoration](https://link.springer.com/article/10.1007/s11263-022-01665-x)
- Computer vision for art analysis
- Digital humanities applications

**Sustainability/Materials:**
- [Material perception and behavior](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2023.1307467/full)
- Sustainable design choices
- Circular economy in design
:::
:::

. . .

**All of these are publishable** — if the question is clear and the method is rigorous.

# Types of Research Questions {background-color="#4ECDC4"}

## Three Main Types

::: {.columns}
::: {.column width="33%"}
### Descriptive

*What is happening?*

- Maps a phenomenon
- Documents experiences
- Establishes baseline

**Example:**
"What are the main frustrations HK design students report when using AI tools for coursework?"

**Methods:** Surveys, interviews, observation
:::

::: {.column width="33%"}
### Relational

*What goes with what?*

- Explores associations
- Identifies patterns
- Doesn't prove causation

**Example:**
"Is there a relationship between time spent on Instagram and self-reported productivity among students?"

**Methods:** Correlational surveys, analytics
:::

::: {.column width="33%"}
### Causal

*Does X cause Y?*

- Tests interventions
- Requires control
- Strongest claims

**Example:**
"Does implementing a 'slow feed' feature reduce anxiety compared to standard feed?"

**Methods:** Experiments, A/B tests
:::
:::

---

## Matching Question Type to Your Project

::: {.callout-note icon=false}
## For a One-Semester Project

**Descriptive and relational questions** are usually most feasible.

Causal questions require controlled experiments, which may be difficult to set up in your timeframe. But you can do exploratory work that *suggests* causal relationships.
:::

. . .

**Think about your Doc 0 topic:**

::: {.incremental}
- What type of question are you asking?
- Is it descriptive (what's happening)?
- Is it relational (what correlates)?
- Is it causal (what causes what)?
- Is the question type realistic for your resources?
:::

# Scoping Your Question {background-color="#FF6B6B"}

## The Goldilocks Problem

::: {.columns}
::: {.column width="33%"}
### Too Broad

"How does technology affect design?"

- Can't answer in a lifetime
- No clear method
- No clear outcome

**Result:** Paralysis
:::

::: {.column width="33%"}
### Too Narrow

"Does the color blue on this specific button increase clicks by 3% on Tuesdays?"

- Trivial findings
- Limited contribution
- Who cares?

**Result:** So what?
:::

::: {.column width="33%"}
### Just Right

"How do color choices in CTA buttons affect conversion rates in e-commerce checkout flows?"

- Specific enough to study
- Broad enough to matter
- Clear path to method

**Result:** Actionable insight
:::
:::

---

## Scoping Dimensions

Use these to narrow or broaden your question:

| Dimension | Broader | Narrower |
|-----------|---------|----------|
| **Population** | "Users" | "HK university students aged 18-24" |
| **Context** | "Social media" | "Instagram Stories feature" |
| **Timeframe** | "Over time" | "During a 2-week study period" |
| **Geography** | "Globally" | "In Hong Kong" |
| **Phenomenon** | "User engagement" | "Double-tap behavior on posts" |

. . .

::: {.callout-tip icon=false}
## Rule of Thumb
If you can't imagine how you'd collect data to answer it, it's probably too broad. If the answer seems obvious or trivial, it's probably too narrow.
:::

---

## Live Rating: How Good Is This Research Question? {background-color="#FFE66D"}

::: {.callout-note icon=false}
## Mentimeter Time

Go to **menti.com** and enter the code on screen.

Rate each research question from **1** (weak) to **5** (strong). Trust your instincts — we'll discuss after.
:::

---

## Rate Question 1

> "How does social media affect teenagers?"

**Rate 1–5 on Mentimeter now.**

. . .

::: {.callout-warning icon=false}
## Verdict: 1–2 (Too Broad)
Which platform? Which teens? Affect *what*? This is a **topic**, not a question. You could write ten dissertations and still not "answer" this.
:::

---

## Rate Question 2

> "How does Instagram's Explore algorithm affect content discovery satisfaction among HK university students?"

**Rate 1–5 on Mentimeter now.**

. . .

::: {.callout-tip icon=false}
## Verdict: 4–5 (Well Scoped)
Specific platform (Instagram Explore), specific population (HK university students), specific outcome (content discovery satisfaction). You can immediately imagine a survey or interview study.
:::

---

## Rate Question 3

> "Does the third post in Instagram Explore get more engagement than the fourth post?"

**Rate 1–5 on Mentimeter now.**

. . .

::: {.callout-warning icon=false}
## Verdict: 1–2 (Too Narrow)
Technically answerable, but so what? The finding would be trivial. No design implication, no theoretical contribution. This is an **A/B test detail**, not a research question.
:::

---

## Rate Question 4

> "What barriers do first-generation university students in Hong Kong face when using AI writing tools for academic coursework?"

**Rate 1–5 on Mentimeter now.**

. . .

::: {.callout-tip icon=false}
## Verdict: 4–5 (Strong)
Defined population (first-gen HK students), specific context (AI writing tools + coursework), exploratory but bounded. Descriptive question with clear interview/survey methodology path.
:::

---

## Rate Question 5

> "Is AI good or bad for education?"

**Rate 1–5 on Mentimeter now.**

. . .

::: {.callout-warning icon=false}
## Verdict: 1 (Not a Research Question)
Binary framing, no defined population, no context, no measurable outcome. This is an **op-ed prompt**, not a research question. It's what you'd argue about at dinner, not in a paper.
:::

---

## What Did We Learn from the Ratings?

::: {.incremental}
- The class mostly **agrees** on what's strong and what's weak — you already have instincts for this
- The hard part isn't recognizing good questions — it's **writing your own** at that level
- Notice the pattern: strong questions have **specific who, what, where** — weak ones are missing at least two
:::

# Activity Part 1: Test Your Signals {background-color="#FFE66D"}

## Your Week 3 Signals — Do They Hold Up? (30-40 min)

Last week you collected quick signals: Reddit threads, App Store reviews, Google Trends, Wayback Machine.

**Those signals told you what people *feel*. Now let's find out if researchers can tell you *why*.**

This is the move from "interesting hunch" to "evidence-backed direction" — and it's exactly what separates a forgettable product pitch from one that makes people lean in.

---

## The Exercise: Deepen 2 Items

::: {.nonincremental}
1. **Pick 2 signals** from your Week 3 5-minute stack
   - A Reddit thread that revealed a pain point
   - An app review pattern you noticed
   - A Google Trends spike that seemed meaningful

2. **For each signal, find 2 academic sources** that study the same phenomenon
   - Use Google Scholar, Semantic Scholar, or Connected Papers
   - Look for studies that explain *why* the pattern exists
:::

---

## For Each Academic Source, Document:

| Component | What to Write |
|:----------|:--------------|
| **Citation** | Author (Year). Title. Journal. |
| **Method** | How did they study this? (Survey? Experiment? Interviews?) |
| **Key Finding** | One sentence: What did they discover? |
| **Limitation** | One sentence: What couldn't they answer? What's still unknown? |
| **Source Quality** | Why trust this? (Peer-reviewed? Citation count? Reputable journal?) |

**This forces rigor.** You can't just grab the first Google Scholar hit. You have to evaluate *why* this source is credible and acknowledge *gaps*.

::: {.callout-tip icon=false}
## Use Your Toolkit
This activity maps directly to **Sections 3–4 of the Landscape Intelligence Canvas**. Use the canvas as your working document — it's designed for exactly this progression.
:::

---

## Example 1: App/Product — From Reddit to Research

::: {.columns}
::: {.column width="40%"}
**Week 3 Signal:**

Reddit r/socialanxiety thread: "I check my likes 20+ times after posting and feel terrible every time"

*Quick insight: compulsive checking exists and causes distress*
:::

::: {.column width="60%"}
**Week 4 Deepening:**

**Source 1:** Vogel et al. (2014). Social comparison, social media, and self-esteem. *Psychology of Popular Media Culture.*

- **Method:** Survey of 145 college students
- **Finding:** Upward social comparison on Facebook linked to lower self-esteem
- **Limitation:** Cross-sectional; can't prove causality
- **Quality:** 1,400+ citations; peer-reviewed; foundational paper in the field

**Source 2:** Verduyn et al. (2017). Passive social media use undermines affective well-being. *Journal of Experimental Psychology.*

- **Method:** Experience sampling over 2 weeks (N=89)
- **Finding:** Passive use (scrolling, comparing) → negative affect
- **Limitation:** Small sample; Facebook only, not Instagram
- **Quality:** APA journal; rigorous ESM methodology
:::
:::

---

## Example 2: Built Environment — From Observation to Research

::: {.columns}
::: {.column width="40%"}
**Week 3 Signal:**

Google Maps reviews of HK public spaces: recurring complaints about "too many rules," "security guards told us to leave," "can't sit on the grass"

*Quick insight: rule enforcement affects how people use public space*
:::

::: {.column width="60%"}
**Week 4 Deepening:**

**Source 1:** Xue et al. (2019). Influence of rules on user behavior in public open space of Hong Kong. *American Journal of Civil Engineering and Architecture.*

- **Method:** Observation + behavioral mapping in 4 HK plazas
- **Finding:** Explicit rules (signs, barriers) significantly reduced dwelling time and social activities
- **Limitation:** Only studied privately-owned public spaces (POPS)
- **Quality:** Peer-reviewed; HK-specific context; cited in urban design lit

**Source 2:** Low (2000). On the Plaza: The Politics of Public Space and Culture. *University of Texas Press.*

- **Method:** Ethnography across multiple US plazas
- **Finding:** Design and management decisions encode social control and exclusion
- **Limitation:** US context; may not transfer to HK cultural norms
- **Quality:** Seminal text in public space studies; 2,000+ citations
:::
:::

---

## Your Deliverable

**By end of class:**

::: {.nonincremental}
1. **1 signal** from your 5-minute stack (named and described)
2. **2 academic sources** with method, finding, limitation, and source quality
3. **Draft gap statement:** What's still unknown based on what you've read?
:::

**Take-home thinking (optional but recommended):**

::: {.nonincremental}
4. Second signal + 2 more sources (same format)
5. Refined gap statement incorporating both signals
6. Research question that emerges from the landscape
7. Method fit: What approach would let you address this question?
:::

**If you continue outside class, bring your working Canvas (Sections 3-4) to Week 5 — it'll feed directly into your Doc 0.1.**

---

## Why This Matters

::: {.callout-important icon=false}
## The Credibility Bridge

**Shallow signals** tell you what to look for.

**Academic sources** tell you whether your instinct has scientific grounding.

**The combination** = a research question that's both relevant (people care) AND rigorous (literature supports it).

This is exactly what product builders need: behavioral science backing for their intuition.
:::

---

## Work Time: Start Here, Continue Later

::: {.callout-note icon=false}
## Realistic Pacing

**In class (25-30 min):** Complete **1 signal + 2 sources** fully documented.

**Take-home thinking:** Continue with your **second signal + 2 sources** at your own pace.

This gives you time to actually read abstracts and evaluate quality — not just grab the first hit.
:::

::: {.callout-tip icon=false}
## Tools to Use

- **Google Scholar** — broad search, citation counts
- **Semantic Scholar** — AI-powered relevance ranking
- **Connected Papers** — visual mapping of related work
- **Elicit** — AI research assistant (use to verify, not replace)

**Pro tip:** If your 5-minute signal doesn't have academic coverage, that itself is a gap worth noting — and potentially a research opportunity.
:::

# Activity Part 2: Build Your Compass {background-color="#4ECDC4"}

## Draft Your Research Question (20 minutes)

You've rated questions, you've deepened your signals. Now build yours. **Working individually or in pairs:**

::: {.nonincremental}
1. **Start with your Doc 0 topic/gap** (or the idea you're considering)

2. **Draft a research question** using this format:
   - "How does [X] affect/relate to [Y] among [population] in [context]?"
   - OR "What [factors] contribute to [outcome] for [population]?"

3. **Check your question against:**
   - Is it specific enough? (Can you imagine collecting data?)
   - Is it broad enough? (Does the answer matter?)
   - Is it feasible? (Can you do this in one semester?)
   - What type is it? (Descriptive/Relational/Causal)

4. **Prepare to share:** Your question + one concern you have about it
:::

::: {.callout-tip icon=false}
## Your Toolkit

**[Research Question Builder (PDF)](data/research_question_builder.pdf)** — FINER criteria checklist, scope check, question type identifier. Use it as your working sheet right now. Page 1 is blank, page 2 is a worked example.
:::

---

## Peer Feedback (10 minutes)

**Share your question with a neighbor. Give feedback:**

::: {.nonincremental}
- Can you understand the question on first reading?
- Is it too broad or too narrow?
- What method might answer this? (If you can't imagine one, it may be too vague)
- What's one way to make it sharper?
:::

. . .

::: {.callout-tip icon=false}
## Feedback Frames
- "I think the question is clear, but I'm not sure how you'd measure [X]..."
- "Could you narrow the population to make it more feasible?"
- "What if you focused on [specific aspect] instead of [broad area]?"
:::

---

## Debrief: Let's Hear Some Questions

::: {.incremental}
- What questions did you come up with?
- What was hardest about formulating the question?
- How did peer feedback help?
:::

. . .

::: {.callout-important icon=false}
## Remember
Your research question will evolve. The literature review (Doc 0.1) often reveals that your question needs adjusting. That's normal and expected.
:::

# Where Product Thinking Comes From {background-color="#FF6B6B"}

## Every Feature You Love Started as a Research Question

You don't need to publish at CHI. But you should know that **CHI is where the playbook gets written** — usually 3–5 years before it shows up in your favorite app.

::: {.incremental}
- **Pull-to-refresh, infinite scroll, notification red dots** — variable ratio reinforcement (Skinner, 1957; operant conditioning research)
- **Every thumb-reachable button you've ever tapped** — Fitts's Law (1954, experimental psychology)
- **"Are you sure you want to close this tab?"** — loss aversion (Kahneman & Tversky, prospect theory)
- **Default opt-in for organ donation, newsletter signups, privacy settings** — nudge theory (Thaler & Sunstein, behavioral economics)
- **The entire engagement model of social media** — the "attention economy" (Herbert Simon coined it in academia)
:::

---

## The Silicon Valley Pipeline You Don't See

::: {.columns}
::: {.column width="50%"}
### The Research Side

- **BJ Fogg** (Stanford HCI) publishes *Persuasive Technology* (2003)
- Concepts: triggers, motivation, ability, behavior loops

- **Kramer et al.** publish Facebook's emotional contagion study (PNAS, 2014)
- Finding: platforms can shift users' moods at scale without their awareness
:::

::: {.column width="50%"}
### The Product Side

- Instagram, Snapchat, TikTok hire Fogg's students → engagement playbook
- Growth hacking, dark patterns, gamification — all downstream

- EU and US regulators begin drafting platform accountability laws
- "Move fast and break things" becomes a liability, not a motto
:::
:::

. . .

::: {.callout-important icon=false}
## The Point
The researchers asked the question. The product teams shipped the answer. The PMs who understood *both* sides made the best decisions — and avoided the worst ones.
:::

---

## Why This Matters for Your Career

::: {.callout-note icon=false}
## Two Types of Product People

**PM who reads Medium:**
"Let's add gamification because Duolingo does it."

**PM who reads the research:**
"Duolingo's streak mechanic works because of commitment-consistency bias (Cialdini, 1984) and variable reward schedules — but the same mechanics cause anxiety in health apps. Our context is different. Here's what the evidence says we should do instead."
:::

. . .

**Which one gets promoted? Which one avoids shipping something harmful?**

The research-literate PM doesn't need to *publish* papers. They need to **read them, evaluate them, and apply them.** That's exactly what this course teaches.

---

## Recommended Reading

::: {.callout-tip icon=false}
## For the Curious

[How to Write a CHI Paper (Asking for a Friend)](https://arxiv.org/html/2401.05818v1) — Even if you never submit to CHI, this shows you how the best HCI researchers think about clarity, contribution, and rigor. The standards they use are the same standards that separate sharp product thinking from hand-waving.
:::

---

## Your Research Question Is Your Foundation

::: {.callout-note icon=false}
## Same Skill, Different Output

**In academia:**
Clear RQ → Clear method → Clear findings → Publication

**In industry:**
Clear problem definition → Clear evidence gathering → Clear insight → Better product decision

The skill is identical. The output is different. Both start with a sharp question.
:::

. . .

**This is why we're spending a whole session on this.**

Whether you end up writing papers or writing PRDs — a vague question leads to vague outcomes.

# Looking Ahead {background-color="#457B9D"}

## From Question to Method

Once you have a clear research question, you can ask:

::: {.incremental}
- **What data would answer this?** (numbers? words? observations?)
- **Who has this data?** (users? databases? you need to collect it?)
- **How would you get it?** (surveys? interviews? experiments?)
- **What would "an answer" look like?** (statistics? themes? comparison?)
:::

. . .

This is exactly what **Doc 0.1 (Literature Review)** prepares you for—and what **Doc 0.2 (Collecting Evidence)** will execute.

---

## The Research Question Cascade

::: {.callout-note icon=false}
## Your Question Shapes Everything

**Research Question** ↓

**Literature Review** — What's already known about this question?

**Methodology** — How will I answer this question?

**Data Collection** — What evidence addresses this question?

**Analysis** — What does the evidence say?

**Conclusion** — Did I answer the question?
:::

A well-formed question makes every subsequent step clearer.

---

## Your Documents Are Version Updates, Not Final Drafts

::: {.callout-important icon=false}
## How the Doc Pipeline Actually Works

Think of it like software releases:

| Doc | Version | What It Is | What Happens Later |
|-----|---------|------------|-------------------|
| **Doc 0** | v0.1 | Your first pitch — rough, instinct-driven | Gets refined by landscape + lit review |
| **Doc 0.1** | v0.2 | Your lit review — first attempt at synthesis | Gets sharper as you learn methods |
| **Doc 0.2** | v0.3 | Your methodology — how you'll investigate | Gets validated by actual data collection |
| **Doc 1** | v1.0 | Your research paper — **contains upgraded versions of everything above** | The release candidate |
:::

. . .

**You are not expected to get Doc 0.1 perfect.** You're expected to get it *done* — and then make it better when you know more. Doc 1 is where the polished versions live.

This is how real research works. Nobody writes a perfect literature review on the first pass. You write it, learn more, revise it, and the final paper is stronger because of the iterations.

---

## Upcoming Deadlines

::: {.columns}
::: {.column width="50%"}
### Just Submitted
**Doc 0: Extended Abstract**

- Feedback coming within the week
- Use it to refine your question
- If you need to adjust — that's the process working, not a failure
:::

::: {.column width="50%"}
### Coming Up
**Doc 0.1: Literature Review**

- Due: Week 6 (Mar 3)
- Your research question is now your search filter
- Write your best v0.2 — you'll upgrade it for Doc 1
:::
:::

. . .

::: {.callout-tip icon=false}
## After the Break: Methods I
We'll cover qualitative research methods — interviews, observations, ethnography. **Bring your research question to class.** We'll use it.
:::

# Key Takeaways {background-color="#457B9D"}

## What We Learned Today

::: {.nonincremental}
1. **Topic ≠ Gap ≠ Question** — A research question is specific and answerable, not just an area of interest

2. **Good questions are FINER** — Feasible, Interesting, Novel, Ethical, Relevant

3. **Three types of questions** — Descriptive (what?), Relational (what correlates?), Causal (what causes?)

4. **Scope matters** — Not too broad (can't answer), not too narrow (doesn't matter)

5. **Your question shapes everything** — It determines your methods, data, and what counts as an answer

6. **Diverse research is valid** — Product, urban, computational, cultural — all publishable with clear questions

7. **Research literacy is a career advantage** — The best product people trace features back to the science that makes them work
:::

---

## The Bridge: Where We're Going

::: {.columns}
::: {.column width="50%"}
**Today (Week 4)**

You learned to formulate a research question

- From gap to inquiry
- Specific and answerable
- Guides your methodology
:::

::: {.column width="50%"}
**Next Week (Week 5)**

You'll learn qualitative methods

- Interviews, observations, ethnography
- How to collect rich, descriptive data
- Which method fits your question?
:::
:::

. . .

**The semester arc continues:**

Instinct → Gap → Landscape → **Question** → Methods → Evidence → Analysis → Paper

---

---

## The Slide to Screenshot

::: {.r-fit-text}
**A vague question**

**leads to a vague search**

**leads to a vague review**

**leads to a vague paper.**
:::

. . .

Sharpen your question. Everything downstream gets easier.

---

## Questions?

::: {.r-fit-text}
Let's discuss!
:::

**Office hours:** By appointment

**Next week:** Methods I — Qualitative Research

::: {.notes}
- Encourage questions about formulating RQs
- Remind them Doc 0.1 builds on today's work
- The question they formulate today guides their lit review search
:::
