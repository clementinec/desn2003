---
title: "DESN2003: Reserach for Innovation"
format: html
---

**Title:**  
Investigating User Feedback for Enhancing GenAI-Integrated Conversational Interfaces in HCI

---

### Abstract

This study looks at how user feedback can help improve conversational interfaces that use generative AI (GenAI) in human-computer interaction (HCI). The idea is to explore both what users say and what their actions suggest about the interface’s design. We aim to understand if the interface meets users’ expectations and what improvements can be made. Early observations indicate that while users appreciate the advanced AI capabilities, there is some confusion regarding the AI’s decision-making process, which might be due to insufficient explanations in the design. The study will combine survey responses with usability testing data to offer recommendations for future improvements.

---

### Literature Review

Human-computer interaction has been evolving over the past few decades, with a lot of focus on making systems more user-friendly. Early research by Norman (2013) showed that usability is the core of good design. More recent studies, however, have started to look at how GenAI can be integrated into these systems to make interactions more natural. For example, Lee and Park (2022) describe conversational agents that can mimic human dialogue, which seems promising but also raises questions about reliability and user trust.

Some researchers argue that the use of GenAI might actually hinder user experience if the technology does not align with user expectations. Johnson et al. (2023) point out that while GenAI can produce quick responses, these responses sometimes lack context, which may lead users to feel misunderstood. This study is important because it mixes descriptive observations—like noting the speed of the responses—with analytical questions about whether the speed compromises the quality of interactions.

There is a noticeable gap, though, in the literature when it comes to understanding how different user groups (novices versus experienced users) actually interact with these systems. Many studies provide a lot of descriptive data (e.g., error rates, response times) without delving into why users might behave in a certain way. This gap is significant because without analysis, we just have numbers without an understanding of the underlying issues.

It is also worth noting that some of the literature is a bit redundant. Many papers just restate that user feedback is important without explaining how to implement it effectively. This is something that our research intends to address by not only gathering descriptive data but also engaging in a deeper analysis of user comments and behavior during interactions with the system.

---

### Methodology

To tackle the research questions, this study employs a mixed-methods approach, even though the approach might seem a bit scattered at first glance. The research is split into three phases.

**Phase 1: Pre-Test Survey**  
- **Participants:** We plan to recruit about 30 undergraduate students, preferably with varying experiences in using conversational interfaces.  
- **Survey Design:** The survey will ask participants about their familiarity with GenAI, their initial impressions of conversational agents, and any previous experiences with AI-based interfaces.  
- **Goal:** This phase is meant to collect baseline data, which is crucial, even if it might seem a bit generic. The data will later be compared with what we observe during usability testing.

**Phase 2: Usability Testing**  
- **Test Setup:** The usability tests will be conducted in a lab setting where participants interact with a prototype conversational interface. The interface is designed to simulate a GenAI-powered dialogue system, though there may be issues with its responsiveness.  
- **Task Scenarios:** Participants will perform tasks like asking for help, checking information, and engaging in casual conversation. These tasks are designed to mimic real-life usage but might not capture every nuance of natural conversation.  
- **Data Collection:** We will collect quantitative data such as task completion time and error rates, as well as qualitative data in the form of recorded verbal feedback and written comments.  
- **Analytical Focus:** The analysis will focus on identifying common themes in the feedback. For example, if several users mention that the system’s responses seem “robotic” or “hard to follow,” we will consider whether the interface lacks sufficient context or explanation. This phase is both descriptive (measuring performance metrics) and analytical (interpreting feedback to suggest improvements).

**Phase 3: Post-Test Survey and Group Discussion**  
- **Post-Test Survey:** After the usability testing, participants will complete a short survey asking for their impressions of the interface and suggestions for improvement.  
- **Group Discussion:** A focus group will be held to discuss the usability test outcomes. The discussion will be semi-structured, allowing for both free-form feedback and guided questions about what worked and what did not.  
- **Data Analysis:** We plan to use thematic analysis on the qualitative data to extract key issues. The intent is to correlate these themes with the quantitative data from Phase 2, though this may lead to some unexpected or messy findings that will need careful interpretation.

---

### Diagnostic Reflection

- **Strengths Noted:**  
  - The writeup successfully combines descriptive details (e.g., the steps of the usability test) with analytical commentary (e.g., questioning why users find responses robotic).  
  - There is a clear attempt to connect the literature with the research methodology.

- **Caveats and Areas for Improvement:**  
  - The language is sometimes repetitive, and the analytical parts are not always fully developed. For instance, the discussion on the gap in literature is somewhat superficial.  
  - There is a mix of overly generic phrases ("data is crucial") with more detailed analysis, which might confuse the reader about the depth of the research.  
  - The structure could be tightened; some parts seem scattered, reflecting a common issue in student writing where descriptive and analytical elements are not clearly separated.

This sample serves as a diagnostic example. It demonstrates both the potential for blending descriptive and analytical writing and the common pitfalls—such as redundancy, superficial analysis, and structural disorganization—that you should work on when revising your own work.

